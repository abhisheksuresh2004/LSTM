{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8+6XXzBkuFw4J/ioUKJRR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhisheksuresh2004/LSTM/blob/main/qGAN%20for%20digital%20forensics\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLFoayuLH7cv",
        "outputId": "92d54815-a363-4aa6-eb15-552b21dedddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.37.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n",
            "Collecting rustworkx (from pennylane)\n",
            "  Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting semantic-version>=2.7 (from pennylane)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting autoray>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.6.12-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\n",
            "Collecting pennylane-lightning>=0.37 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.1)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.7.4)\n",
            "Downloading PennyLane-0.37.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.6.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PennyLane_Lightning-0.37.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading rustworkx-0.15.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, semantic-version, rustworkx, autoray, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.6.12 pennylane-0.37.0 pennylane-lightning-0.37.0 rustworkx-0.15.1 semantic-version-2.10.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "!pip install pennylane\n",
        "import pennylane as qml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pennylane as qml\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Quantum variables\n",
        "n_qubits = 12  # Updated to generate enough values for reshaping\n",
        "q_depth = 6  # Depth of the parameterized quantum circuit / D\n",
        "n_generators = 4  # Number of sub-generators for the patch method / N_G\n",
        "\n",
        "# Quantum simulator\n",
        "dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev, interface=\"tf\", diff_method=\"parameter-shift\")\n",
        "def quantum_circuit(noise, weights):\n",
        "    weights = tf.reshape(weights, (q_depth, n_qubits))\n",
        "\n",
        "    # Initialize latent vectors\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(noise[i], wires=i)\n",
        "\n",
        "    # Repeated layer\n",
        "    for i in range(q_depth):\n",
        "        # Parameterized layer\n",
        "        for y in range(n_qubits):\n",
        "            qml.RY(weights[i][y], wires=y)\n",
        "\n",
        "        # Control Z gates\n",
        "        for y in range(n_qubits - 1):\n",
        "            qml.CZ(wires=[y, y + 1])\n",
        "\n",
        "    return qml.probs(wires=list(range(n_qubits)))\n",
        "\n",
        "def partial_measure(noise, weights):\n",
        "    # Non-linear Transform\n",
        "    probs = quantum_circuit(noise, weights)\n",
        "\n",
        "    # Select the first 3072 values to match the required shape\n",
        "    probs = probs[:3072]\n",
        "\n",
        "    # Normalize and process\n",
        "    probs /= tf.reduce_sum(probs)\n",
        "    probs = probs / tf.reduce_max(probs)\n",
        "\n",
        "    return tf.cast(probs, tf.float32)\n",
        "\n",
        "class PatchQuantumGenerator(tf.keras.Model):\n",
        "    \"\"\"Quantum generator class for the patch method\"\"\"\n",
        "\n",
        "    def __init__(self, n_generators, q_delta=1):\n",
        "        super(PatchQuantumGenerator, self).__init__()\n",
        "\n",
        "        # Register q_params as trainable weights using `self.add_weight`\n",
        "        self.q_params = []\n",
        "        for _ in range(n_generators):\n",
        "            self.q_params.append(\n",
        "                self.add_weight(\n",
        "                    shape=(q_depth * n_qubits,),\n",
        "                    initializer=tf.random_uniform_initializer(minval=-q_delta, maxval=q_delta),\n",
        "                    trainable=True\n",
        "                )\n",
        "            )\n",
        "        self.n_generators = n_generators\n",
        "\n",
        "        # Resizing layer to match the discriminator's input shape\n",
        "        self.resize_layer = tf.keras.layers.Resizing(224, 224)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = tf.cast(x, tf.float32)  # Ensure input is float32\n",
        "\n",
        "        # Process all inputs simultaneously (vectorized approach)\n",
        "        patches = []\n",
        "        for params in self.q_params:\n",
        "            batch_outputs = []\n",
        "            for i in range(x.shape[0]):  # Batch size\n",
        "                batch_outputs.append(partial_measure(x[i], params))\n",
        "            patches.append(tf.stack(batch_outputs, axis=0))\n",
        "\n",
        "        # Concatenate the patches along the last dimension to form the final images\n",
        "        images = tf.concat(patches, axis=-1)\n",
        "\n",
        "        # Adjust the reshape based on available values\n",
        "        required_shape = (x.shape[0], 32, 32, 3)  # Ensure 3 channels\n",
        "        images = tf.reshape(images, required_shape)\n",
        "\n",
        "        # Resize output to match (batch_size, 224, 224, 3)\n",
        "        images = self.resize_layer(images)\n",
        "        return images\n",
        "\n",
        "class ClassicalDiscriminator(tf.keras.Model):\n",
        "    \"\"\"Classical Discriminator Model\"\"\"\n",
        "    def __init__(self):\n",
        "        super(ClassicalDiscriminator, self).__init__()\n",
        "        self.model = tf.keras.Sequential([\n",
        "            layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same', input_shape=(224, 224, 3)),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'),\n",
        "            layers.LeakyReLU(alpha=0.2),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Load the IFrame dataset\n",
        "data_dir = \"/content/drive/My Drive/Samsung\"  # Replace with your path\n",
        "img_height, img_width = 224, 224  # Image size\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Loss functions and optimizers\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator = PatchQuantumGenerator(n_generators)\n",
        "discriminator = ClassicalDiscriminator()\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.uniform([batch_size, n_qubits])  # Adjust input shape as needed\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise)\n",
        "\n",
        "        real_output = discriminator(images)\n",
        "        fake_output = discriminator(generated_images)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "# Debugging: Check generator trainable variables\n",
        "print(\"Generator trainable variables:\", generator.trainable_variables)\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for image_batch in dataset:\n",
        "            gen_loss, disc_loss = train_step(image_batch)\n",
        "        print(f'Epoch {epoch + 1}, Gen Loss: {gen_loss.numpy()}, Disc Loss: {disc_loss.numpy()}')\n",
        "\n",
        "# Start training\n",
        "train(train_ds, epochs=50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81nRKbohytXP",
        "outputId": "0ba18d66-ee21-4929-c17f-1096e069e37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 2968 files belonging to 2 classes.\n",
            "Using 2375 files for training.\n",
            "Found 2968 files belonging to 2 classes.\n",
            "Using 593 files for validation.\n",
            "Generator trainable variables: [<KerasVariable shape=(72,), dtype=float32, path=patch_quantum_generator/variable>, <KerasVariable shape=(72,), dtype=float32, path=patch_quantum_generator/variable_1>, <KerasVariable shape=(72,), dtype=float32, path=patch_quantum_generator/variable_2>, <KerasVariable shape=(72,), dtype=float32, path=patch_quantum_generator/variable_3>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}